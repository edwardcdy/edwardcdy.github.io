---
title: "Resource List"
date: 2022-01-01
layout: post
---

This page will be a master list of resources (e.g. video lectures, books, papers, etc.). Most of these should just be useful links, the actual content and my summaries will be in posts.



# General Deep Learning

- [Deep learning course - NYU '21](https://www.youtube.com/watch?v=mTtDfKgLm54&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI). Taught by Yann LeCun(!) and Alfredo Canziani, and hosted on the latter's youtube channel. IMO this whole series is highly worth a watch if you have time, even if you are a seasoned DL researcher (especially Yann's lectures). 
- [Statistical Machine Learning - Tubingen '20](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC). This is traditional machine learning, so very low on my priorities list to finish, but looks interesting.
- [Deep learning for NLP - Stanford '21](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ) A classic course from Stanford, one of many years with video lectures on Youtube.






# Computer Vision

### Papers

- [[He '21]](https://arxiv.org/pdf/2111.06377v2.pdf) Masked Autoencoders Are Scalable Vision Learners
- [[Park '20]](https://arxiv.org/pdf/2007.15651.pdf) Contrastive Learning for Unpaired Image-to-Image Translation
- [[Dosovitsky '20]](https://arxiv.org/pdf/2010.11929.pdf) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- [[Tolstikhin '21]](https://arxiv.org/pdf/2105.01601.pdf) MLP-Mixer: An all-MLP Architecture for Vision
- [[Bardes '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=5c4f975ab79611ebbd9b8f626bc6f333) VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning
- [[Chen '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=4baf86741c1111ec9e9dcba33be64600) Pix2seq: A Language Modeling Framework for Object Detection
- [[Liu '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=9f4eeafc728311ec905035ac8856fc87) A ConvNet for the 2020s
- [[He '15]](https://arxiv.org/pdf/1512.03385.pdf) Deep Residual Learning for Image Recognition (aka **resnet**)
- [[Huang '17]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf) Densely Connected Convolutional Networks
- [[Redmon '16]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) You Only Look Once: Unified, Real-Time Object Detection
- [[Reed '16]](http://proceedings.mlr.press/v48/reed16.pdf) Generative Adversarial Text to Image Synthesis
- [[Oord '16]](http://proceedings.mlr.press/v48/oord16.pdf) Pixel Recurrent Neural Networks
- [[Oord '16]](https://arxiv.org/pdf/1606.05328.pdf) Conditional Image Generation with PixelCNN Decoders
- [[Zhang '19]](http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf) Self-Attention Generative Adversarial Networks
- [[Gatys '19]](https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) Image Style Transfer Using Convolutional Neural Networks

# Graph Neural Networks

### Papers
- [[Kipf '17]](https://arxiv.org/pdf/1609.02907.pdf) Semi-Supervised Classification with Graph Convolutional Networks.
- [[Velickovic '17]](https://arxiv.org/pdf/1710.10903.pdf) Graph Attention Networks


# Reinforcement Learning

### Papers

- [[Lillicrap '15]](https://arxiv.org/pdf/1509.02971.pdf) Continuous control with deep reinforcement learning.




# Natural Language Processing

### Papers

- [[Devlin '19]](https://arxiv.org/pdf/1810.04805.pdf) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- [[Bahdanau '15]](https://arxiv.org/pdf/1409.0473.pdf) Neural Machine Translation by Jointly Learning to Align and Translate
- [[Miller '16]](https://arxiv.org/pdf/1606.03126.pdf) Key-Value Memory Networks for Directly Reading Documents
- [[Vaswani '17]](https://arxiv.org/pdf/1706.03762.pdf) Attention is all you need
- [[Xie '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=762cac04c27911eb80dc0bd1877e23b6) SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers
- [[Lan '19]](https://arxiv.org/pdf/1909.11942.pdf) ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
- [[Amodei '15]](https://arxiv.org/pdf/1512.02595.pdf) Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
- [[Sutskever '14]](https://arxiv.org/pdf/1409.3215.pdf) Sequence to Sequence Learning with Neural Networks
- [[Graves '12]](https://arxiv.org/pdf/1211.3711.pdf) Sequence Transduction with Recurrent Neural Networks
- [[Mikolov '13]](https://arxiv.org/pdf/1301.3781.pdf) Efficient Estimation of Word Representations in Vector Space
- [[Mikolov '13]](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) Distributed Representations of Words and Phrases and their Compositionality
- [[Bengio '94]](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf) Learning Long-Term Dependencies with Gradient Descent is difficult
- [[Luong '16]](https://arxiv.org/pdf/1604.00788.pdf) Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models
- [[Huang '18]](https://arxiv.org/pdf/1809.04281.pdf) Music Transformer: Generating music with long-term structure
- [[Radford '18]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) Improving Language Understanding by Generative Pre-Training
- [[Peters '18]](https://arxiv.org/pdf/1802.05365.pdf) Deep contextualized word representations
- [[Liu '19]](https://arxiv.org/pdf/1907.11692.pdf) RoBERTa: A Robustly Optimized BERT Pretraining Approach
- [[Bińkowski '19]](https://arxiv.org/abs/1909.11646) High Fidelity Speech Synthesis with Adversarial Networks
- [[Kong '19]](https://arxiv.org/pdf/1910.08350.pdf) A Mutual Information Maximization Perspective of Language Representation Learning



# Miscellaneous

### Papers
- [[Kingma '14]](https://arxiv.org/abs/1312.6114) the VAE paper
- [[Gulrajani '17]](https://arxiv.org/pdf/1704.00028.pdf) Improved Training of Wasserstein GANs
- [[Graves '06]](https://arxiv.org/pdf/1410.5401.pdf) Neural turing machines
- [[Wu '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=d99370b84d9411ec904f35ac8856fc87) NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion
- [[Drori '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=212cd37c6c3a11ec905035ac8856fc87) A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More
- [[Han '15]](https://arxiv.org/pdf/1510.00149.pdf) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding
- [[Salimans '16]](https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf) Improved techniques for training GANs
- [[Ba '16]](https://arxiv.org/pdf/1607.06450.pdf) Layer Normalization

# Meta

- [Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI](https://arxiv.org/ftp/arxiv/papers/2201/2201.00650.pdf)