---
permalink: /resources
layout: page
title: Resource List

---

This page will be a master list of resources (e.g. video lectures, books, papers, etc.). Most of these should just be useful links, the actual content and my summaries will be in posts.



# General Deep Learning


## Papers/books

- [[Mathematics for machine learning]](https://mml-book.github.io/book/mml-book.pdf)

## Video Lectures

- [Deep learning course - NYU '21](https://www.youtube.com/watch?v=mTtDfKgLm54&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI). Taught by Yann LeCun(!) and Alfredo Canziani, and hosted on the latter's youtube channel. IMO this whole series is highly worth a watch if you have time, even if you are a seasoned DL researcher (especially Yann's lectures). 
- [Statistical Machine Learning - Tubingen '20](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC). This is traditional machine learning, so very low on my priorities list to finish, but looks interesting.
- [Deep learning for NLP - Stanford '21](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ) A classic course from Stanford, one of many years with video lectures on Youtube.
- [2019 summer school - math of machine learning](https://www.youtube.com/watch?v=3wbLr-NnIKI&list=PLTPQEx-31JXhguCush5J7OGnEORofoCW9&index=1) Really cool playlist for more theoretical concepts
- [[2020 UT Austin Optimization]](https://www.youtube.com/watch?v=ee-HYD6kKqM&list=PLXsmhnDvpjORzPelSDs0LSDrfJcqyLlZc)
## Websites

- [Depth First Learning](https://www.depthfirstlearning.com/) Self-contained curriculum over various topics, e.g. optimization, the sigmoid, variational inference






# Computer Vision

### Papers

- [[Krizhevsky '12]](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) ImageNet Classification with Deep Convolutional Neural Networks
- [[Szegedy '14]](https://arxiv.org/pdf/1409.4842.pdf) Going deeper with convolutions
- [[Szegedy '15]](https://arxiv.org/pdf/1512.00567.pdf) Rethinking the Inception Architecture for Computer Vision
- [[Vinyals '15]](https://arxiv.org/pdf/1411.4555.pdf) Show and Tell: A Neural Image Caption Generator
- [[He '15]](https://arxiv.org/pdf/1512.03385.pdf) Deep Residual Learning for Image Recognition (aka **resnet**)
- [[Simonyan '15]](https://arxiv.org/pdf/1409.1556.pdf) Very Deep Convolutional Networks for Large-Scale Image Recognition
- [[Ren '16]](https://arxiv.org/pdf/1506.01497.pdf) Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
- [[Redmon '16]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) You Only Look Once: Unified, Real-Time Object Detection
- [[Reed '16]](http://proceedings.mlr.press/v48/reed16.pdf) Generative Adversarial Text to Image Synthesis
- [[Oord '16]](http://proceedings.mlr.press/v48/oord16.pdf) Pixel Recurrent Neural Networks
- [[Oord '16]](https://arxiv.org/pdf/1606.05328.pdf) Conditional Image Generation with PixelCNN Decoders
- [[He '16]](https://arxiv.org/pdf/1603.05027.pdf) Identity Mappings in Deep Residual Networks
- [[Xie '17']](https://arxiv.org/pdf/1611.05431.pdf) Aggregated Residual Transformations for Deep Neural Networks
- [[Qi '17]](https://arxiv.org/pdf/1612.00593.pdf) PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation
- [[Huang '17]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf) Densely Connected Convolutional Networks
- [[He '18]](https://arxiv.org/abs/1703.06870) Mask R-CNN
- [[Gkioxari '19]](https://arxiv.org/pdf/1906.02739.pdf) Mesh R-CNN
- [[Zhang '19]](http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf) Self-Attention Generative Adversarial Networks
- [[Tan '20]](https://arxiv.org/pdf/1905.11946.pdf) EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
- [[Park '20]](https://arxiv.org/pdf/2007.15651.pdf) Contrastive Learning for Unpaired Image-to-Image Translation
- [[Dosovitsky '20]](https://arxiv.org/pdf/2010.11929.pdf) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- [[Tolstikhin '21]](https://arxiv.org/pdf/2105.01601.pdf) MLP-Mixer: An all-MLP Architecture for Vision
- [[Bardes '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=5c4f975ab79611ebbd9b8f626bc6f333) VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning
- [[Chen '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=4baf86741c1111ec9e9dcba33be64600) Pix2seq: A Language Modeling Framework for Object Detection
- [[Liu '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=9f4eeafc728311ec905035ac8856fc87) A ConvNet for the 2020s
- [[He '21]](https://arxiv.org/pdf/2111.06377v2.pdf) Masked Autoencoders Are Scalable Vision Learners







# Graph Neural Networks

### Papers
- [[Kipf '17]](https://arxiv.org/pdf/1609.02907.pdf) Semi-Supervised Classification with Graph Convolutional Networks.
- [[Velickovic '17]](https://arxiv.org/pdf/1710.10903.pdf) Graph Attention Networks


# Reinforcement Learning

### Papers

- [[Lillicrap '15]](https://arxiv.org/pdf/1509.02971.pdf) Continuous control with deep reinforcement learning.




# Natural Language Processing

### Papers
- [[Bengio '94]](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf) Learning Long-Term Dependencies with Gradient Descent is difficult
- [[Graves '12]](https://arxiv.org/pdf/1211.3711.pdf) Sequence Transduction with Recurrent Neural Networks
- [[Graves '13]](https://arxiv.org/abs/1303.5778.pdf) Speech Recognition with Deep Recurrent Neural Networks
- [[Mikolov '13]](https://arxiv.org/pdf/1301.3781.pdf) Efficient Estimation of Word Representations in Vector Space
- [[Mikolov '13]](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) Distributed Representations of Words and Phrases and their Compositionality
- [[Sutskever '14]](https://arxiv.org/pdf/1409.3215.pdf) Sequence to Sequence Learning with Neural Networks
- [[Abdel-Hamid '14]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf) Convolutional neural networks for speech recognition
- [[Bahdanau '15]](https://arxiv.org/pdf/1409.0473.pdf) Neural Machine Translation by Jointly Learning to Align and Translate
- [[Amodei '15]](https://arxiv.org/pdf/1512.02595.pdf) Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
- [[Chorowski '15]](https://arxiv.org/pdf/1506.07503.pdf) Attention-Based Models for Speech Recognition
- [[Zhang '15]](https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf) Character-level Convolutional Networks for Text Classification
- [[Luong '16]](https://arxiv.org/pdf/1604.00788.pdf) Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models
- [[Chan '16']](https://arxiv.org/pdf/1508.01211.pdf) Listen, Attend and Spell
- [[Miller '16]](https://arxiv.org/pdf/1606.03126.pdf) Key-Value Memory Networks for Directly Reading Documents
- [[Vaswani '17]](https://arxiv.org/pdf/1706.03762.pdf) Attention is all you need
- [[Chiu '17]](https://arxiv.org/pdf/1712.01769.pdf) State-of-the-art Speech Recognition With Sequence-to-Sequence Models
- [[Wang '17]](https://arxiv.org/pdf/1703.10135.pdf) Tacotron: Towards End-to-End Speech Synthesis
- [[Huang '18]](https://arxiv.org/pdf/1809.04281.pdf) Music Transformer: Generating music with long-term structure
- [[Radford '18]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) Improving Language Understanding by Generative Pre-Training
- [[Peters '18]](https://arxiv.org/pdf/1802.05365.pdf) Deep contextualized word representations
- [[Radford '19]](http://www.persagen.com/files/misc/radford2019language.pdf) Language Models are Unsupervised Multitask Learners
- [[Devlin '19]](https://arxiv.org/pdf/1810.04805.pdf) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- [[Lan '19]](https://arxiv.org/pdf/1909.11942.pdf) ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
- [[Liu '19]](https://arxiv.org/pdf/1907.11692.pdf) RoBERTa: A Robustly Optimized BERT Pretraining Approach
- [[Bi≈Ñkowski '19]](https://arxiv.org/abs/1909.11646) High Fidelity Speech Synthesis with Adversarial Networks
- [[Kong '19]](https://arxiv.org/pdf/1910.08350.pdf) A Mutual Information Maximization Perspective of Language Representation Learning
- [[Brown 20']](https://arxiv.org/pdf/2005.14165v2.pdf) Language Models are Few-Shot Learners 
- [[Beltagy '20]](https://arxiv.org/pdf/2004.05150.pdf) Longformer: The Long-Document Transformer
- [[Kitaev '20]](https://arxiv.org/pdf/2001.04451.pdf) Reformer: The Efficient Transformer
- [[Conformer '20]](https://arxiv.org/pdf/2005.08100.pdf) Conformer: Convolution-augmented Transformer for Speech Recognition
- [[Xie '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=762cac04c27911eb80dc0bd1877e23b6) SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers



# Miscellaneous

### Papers
- [[Graves '06]](https://arxiv.org/pdf/1410.5401.pdf) Neural turing machines
- [[Kingma '14]](https://arxiv.org/abs/1312.6114) the VAE paper
- [[Srivastava '14]](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) Dropout: A Simple Way to Prevent Neural Networks from Overfitting
- [[Kingma '14]](https://arxiv.org/pdf/1412.6980.pdf) Adam: A Method for Stochastic Optimization
- [[Ioffe '15]](https://arxiv.org/pdf/1502.03167.pdf) Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
- [[Han '15]](https://arxiv.org/pdf/1510.00149.pdf) Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding
- [[Salimans '16]](https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf) Improved techniques for training GANs
- [[Ba '16]](https://arxiv.org/pdf/1607.06450.pdf) Layer Normalization
- [[Chollet '17]](https://arxiv.org/pdf/1610.02357.pdf) Xception: Deep Learning with Depthwise Separable Convolutions
- [[Wu '18]](https://arxiv.org/pdf/1803.08494.pdf) Group Normalization
- [[Hu '18]](https://arxiv.org/pdf/1709.01507.pdf) Squeeze-and-Excitation Networks
- [[McCandlish '18]](https://arxiv.org/pdf/1812.06162.pdf) An Empirical Model of Large-Batch Training
- [[Barratt '18]](https://arxiv.org/pdf/1801.01973.pdf) A Note on the Inception Score
- [[Gidaris '18]](https://arxiv.org/pdf/1803.07728.pdf) Unsupervised Representation Learning by Predicting Image Rotations
- [[Oord '18]](https://arxiv.org/pdf/1807.03748.pdf) Representation Learning with Contrastive Predictive Coding
- [[Recht '19]](https://arxiv.org/pdf/1902.10811.pdf) Do ImageNet Classifiers Generalize to ImageNet?
- [[Frankle '19]](https://arxiv.org/pdf/1803.03635.pdf) The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
- [[Wu '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=d99370b84d9411ec904f35ac8856fc87) N√úWA: Visual Synthesis Pre-training for Neural visUal World creAtion
- [[Drori '21]](https://papers.labml.ai/api/v1/redirect/pdf?paper_key=212cd37c6c3a11ec905035ac8856fc87) A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More



# Unsupervised Learning

### Papers

- [[Goodfellow '15]](https://arxiv.org/pdf/1406.2661.pdf) Generative Adversarial Nets
- [[Dinh '16]](https://arxiv.org/abs/1605.08803) Density estimation using Real NVP
- [[Oord '16]](https://arxiv.org/pdf/1609.03499.pdf) WaveNet: A Generative Model for Raw Audio
- [[Gulrajani '17]](https://arxiv.org/pdf/1704.00028.pdf) Improved Training of Wasserstein GANs
- [[Miyato '18]](https://arxiv.org/pdf/1802.05957.pdf) Spectral Normalization for Generative Adversarial Networks
- [[Brock '18]](https://arxiv.org/abs/1809.11096) Large Scale GAN Training for High Fidelity Natural Image Synthesis
- [[Gatys '19]](https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) Image Style Transfer Using Convolutional Neural Networks
- [[Zhu '20]](https://arxiv.org/pdf/1703.10593.pdf) Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
- [[Kingma '18]](https://arxiv.org/abs/1807.03039) Glow: Generative Flow with Invertible 1x1 Convolutions






# Meta

- [Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI](https://arxiv.org/ftp/arxiv/papers/2201/2201.00650.pdf)